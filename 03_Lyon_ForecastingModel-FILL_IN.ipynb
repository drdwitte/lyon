{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the occupancies of Belgian trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vector/matrix library\n",
    "import numpy as np\n",
    "#data frame library (similar to R)\n",
    "import pandas as pd\n",
    "\n",
    "#visualization library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#regular expression library for data cleasning\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a basic forecasting model\n",
    "\n",
    "### 3.1 Create a simple Kaggle submission\n",
    "\n",
    "1. Create a csv document, just like the random.csv, submit to kaggle and try to get to the baseline performance in the leaderboard\n",
    "2. **OPTIONAL:** what performance can you get by making a manually crafted rule-based prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "dataset = pd.read_csv('json_cleaned.csv')\n",
    "\n",
    "dataset['querytime'] = pd.to_datetime(dataset['querytime'])\n",
    "\n",
    "mask = dataset['id'] < 0\n",
    "training_set = dataset[mask]\n",
    "test_set = pd.DataFrame(dataset[~mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = <FILL_IN>\n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating Numerical Features for sklearn\n",
    "\n",
    "As mentioned in the theory slides: machine learning models can only deal with numerical data. In what follows we'll prepare a minimal dataframe and convert it to numpy arrays (which serve as input to an ML model)\n",
    "\n",
    "Sklearn follows a very simple approach: http://scikit-learn.org/stable/data_transforms.html\n",
    "\n",
    "- Estimators, have a fit method, which learns model parameters from a dataset\n",
    "- Transformers have a transform method which applies this transformation model to unseen data. Making a prediction is considered a transformation, but also scaling the data for example!\n",
    "- fit_transform combines the above\n",
    "    \n",
    "    \n",
    "1. Prepare a dataframe with train_type, direction and a number of time series features (see EDA notebook). Make sure the data is scaled (for better model performance) and encode all categorical variables. Use the methods which are imported from the sklearn.preprocessing module\n",
    "\n",
    "2. Split back into train and test and convert the dataframes to nd.array   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create additional time features: month, weekday, hour,... => some inspiration from the EDA notebook?\n",
    "dataset_prep1 = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode train_direction\n",
    "dataset_prep2 = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map occupancies from [low, medium, high] -> [0,1,2]\n",
    "dataset_prep3 = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the pandas get dummies function, it converts categorical data into a numerical presentation...\n",
    "dataset_prep4 = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the time features for better model performance\n",
    "dataset_prep5 = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate train from test\n",
    "training_set = <FILL_IN>\n",
    "test_set = <FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 First Submission - Simple Linear Regression\n",
    "\n",
    "* Fit a basic Linear Regresssion Model to the data\n",
    "* Predict the labels for the test data\n",
    "* The labels are continuous, choose a range to map to 0, 1 and 2 respectively\n",
    "* Create a submission and upload to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "\n",
    "X_train = <FILL_IN>\n",
    "y_train = <FILL_IN>\n",
    "\n",
    "#train model\n",
    "lin_reg.<FILL_IN>\n",
    "\n",
    "X_test = <FILL_IN>\n",
    "y_pred = lin_reg.<FILL_IN>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def occ_to_int(o):\n",
    "    <FILL_IN>\n",
    "\n",
    "test_set['occupancy'] = test_set['occupancy'].apply(occ_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = <FILL_IN>\n",
    "submission.to_csv('slr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Offline Model Evaluation\n",
    "\n",
    "1. Make a (manual) Train / Test split \n",
    "    * First 90% of the training data is used for model training, the remainder is used for model evaluation\n",
    "    * (K-fold cross validation is more common, do you have an idea why it might not be appropriate here?)\n",
    "2. To evaluate the basic model, plot the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\"> confusion matrix </a> and caculate the accuracy\n",
    "3. Test the previous model for different values of the hyperparameter\n",
    "4. Try a number of different approaches, for example a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\"> k-Nearest Neighbours </a> classifier, definitely also have a look at a tree-based approach: <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\"> Random Forest Classifier </a>\n",
    "5. Which of these models has the highest prediction accuracy? Are they complementary or do their correct predictions mostly overlap? (complementary models can be good candidates for building <a href=\"http://scikit-learn.org/stable/modules/ensemble.html\">ensembles </a>)\n",
    "\n",
    "6. Next steps for the quick students: \n",
    "    * use station locations and merge with to, from columns: stations.csv\n",
    "    * use line stops: lineinfo.csv\n",
    "    * look for holiday information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 split X_train in a part for offline training and a part for offline evaluation (=> you need labeled data!)\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfmatrix = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "cax = ax.imshow(confusion)\n",
    "fig.colorbar(cax)\n",
    "plt.xticks([0,1,2])\n",
    "plt.yticks([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_tr, y_tr)\n",
    "y_rf = rf.predict(X_te)\n",
    "accuracy_score(y_te, y_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5 create datframe with all prediction and the actual solution\n",
    "df_complementary = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NN correct: ' +str(<FILL_IN>))\n",
    "print('RF correct: ' +str(<FILL_IN>))\n",
    "print('LR correct: ' +str(<FILL_IN>))\n",
    "print()\n",
    "print('LR & NN correct: ' +str(<FILL_IN>))\n",
    "print('LR & RF correct: ' +str(<FILL_IN>))\n",
    "print('RF & NN correct: ' +str(<FILL_IN>))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
