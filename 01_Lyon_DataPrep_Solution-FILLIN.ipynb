{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the occupancies of Belgian trains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Tips and tricks Jupyter notebooks\n",
    "\n",
    "* Export your notebooks locally: File -> Download as -> html/ipynb (do both!) (NOTE: don't trust the virtual environment, always download your latest version locally!)\n",
    "\n",
    "* Shift + Enter to run a cell (or play button)\n",
    "\n",
    "* TAB for code completion!\n",
    "\n",
    "* Shells can be Code but also Markdown! (dropdown menu on top allows you to choose)\n",
    "\n",
    "* Cell -> Run All might come in useful\n",
    "\n",
    "* In case of problems you can always Kernel -> Restart\n",
    "\n",
    "* In the Jupyter start window you have a 'new' button on the upper right => to open a terminal, a new python3 notebook, etc.\n",
    "\n",
    "* You cannot upload very large files (>3GB) via the upload in Jupyter (bug), the datasets will always be available via een public Amazon url, so use wget or scp to get your data in /mnt on the virtual wall!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "### Essential Libraries for Python Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vector/matrix library\n",
    "import numpy as np\n",
    "#data frame library (similar to R)\n",
    "import pandas as pd\n",
    "\n",
    "#visualization library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#regular expression library for data cleasning\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Get up to speed with python / pandas\n",
    "\n",
    "* If you are new to python have a look at the practicelab.ipynb in the github repo\n",
    "* If you are new to pandas take some time for:\n",
    "    - <a href=\"http://pandas.pydata.org/pandas-docs/stable/10min.html\"> Pandas basics in 10 minutes </a>\n",
    "    - <a href=\"http://pandas.pydata.org/pandas-docs/stable/visualization.html\"> Pandas for data visualizations </a>\n",
    "    - Use the pandas cheat sheet in the github repo\n",
    "    \n",
    "    \n",
    "#### TIP: Pandas dataframes are immutable, so appending a row creates a copies the old df in the new one => creating a df like this is an O(N^2) operation (I learnt this the hard way)    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Dataset characteristics\n",
    "\n",
    "1. Read the train and test datasets\n",
    "2. How many records and how many features are in train and test sets?\n",
    "3. What is the range of the querytime column (earliest + latest date)\n",
    "4. Merge training and test data (data cleansing will be the same for both)\n",
    "5. Drop columns querytype and user_agent since they contain no useful info, this will speedup future df calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "path_train = <FILL_IN>\n",
    "path_test  = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning the input files are slightly different format, training data is in new-line delimited json, \n",
    "#testdata in regular json\n",
    "df_train = pd.read_json(<FILL_IN>)\n",
    "df_test = pd.read_json(<FILL_IN>)\n",
    "\n",
    "#to have a look at a dataframe just use head or tail\n",
    "df_train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "print(\"Number of records in training set: \" + str(<FILL_IN>))\n",
    "print(\"Number of records in test set: \" + str(<FILL_IN>))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Features training data: \" + str(<FILL_IN>))\n",
    "print(\"Features test data: \" + str(<FILL_IN>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "print('Querytime min (training): ' + str(<FILL_IN>))\n",
    "print('Querytime max (training): ' + str(<FILL_IN>))\n",
    "print()\n",
    "print('Querytime min (test): ' + str(<FILL_IN>))\n",
    "print('Querytime max (test): ' + str(<FILL_IN>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "#merge train and test data AND reset the index\n",
    "dataset_v1 = <FILL_IN>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#5\n",
    "dataset_v2 = <FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Parsing the columns\n",
    "\n",
    "HINT: very often you will take a column an 'apply' a transformation to it. Look up the pandas syntax for apply()!\n",
    "\n",
    "1. In the id column replace the NaNs with -1\n",
    "2. Select a row by id and have a closer look at the post column (json object)\n",
    "    * check multiple rows since the post object doesn't always have the same fields\n",
    "3. Iterate over the frame while keeping track of the possible fields in post => what is the set of ps\n",
    "4. Write a function to extract a field of the json object \n",
    "5. For every field in the json objects create an additional column, finally drop the post column\n",
    "6. The query time column is now interpreted as a string => convert to datetime, the id column can be cast to integer\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "transformed_column = <FILL_IN>\n",
    "dataset_v2['id'] = transformed_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "row = <FILL_IN>\n",
    "post_obj = row['post']\n",
    "post_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "   \n",
    "unique_keys = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4\n",
    "def extract_field(json, field):\n",
    "    <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "dataset_v3 = <FILL_IN>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "dataset_v3['querytime'] = <FILL_IN>\n",
    "dataset_v3['id'] = <FILL_IN>\n",
    "dataset_v3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 And some more cleansing\n",
    "\n",
    "- TASK: go over each of the columns, further parse them and pay close attention to NULL values\n",
    "- HINT: don't mindlessly drop rows with missing data, the dataset has some level of redundancy so pay close attention!\n",
    "\n",
    "- HINT: Look up the pandas isnull() function\n",
    "- HINT: use the value_counts() function to check the distribution of values in a column\n",
    "\n",
    "1. How many NULL values per column?\n",
    "2. Rework the vehicle column:\n",
    "    - use the following information on the slides and at: https://nl.wikipedia.org/wiki/Lijst_van_treincategorie%C3%ABn_in_Belgi%C3%AB\n",
    "    - extract connection type, train series, sequence number and you can also infer the direction of the train\n",
    "    - keep checking for NULLs, some can be disguised\n",
    "\n",
    "4. connection\n",
    "5. date\n",
    "6. occupancy\n",
    "7. to, from columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v3[pd.isnull(dataset_v3['to'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "dataset_v3['vehicle'] = <FILL_IN>\n",
    "\n",
    "#write some helper functions to extract the vehicle features such as train_type, direction, series,..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v4 = dataset_v3.drop(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_v4['train_series']    = <FILL_IN>\n",
    "dataset_v4['train_direction'] = <FILL_IN>\n",
    "dataset_v4['train_type']      = <FILL_IN>\n",
    "\n",
    "dataset_v5 = dataset_v4.drop(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "dataset_v5['occupancy'] = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "dataset_v5['from'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v5['to'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#study the bad records (not only NULL!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_v6 = dataset_v5.drop(<FILL_IN>) #drop bad records in to column\n",
    "dataset_v7 = dataset_v6.drop(<FILL_IN>) #drop bad records in from column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_v7[dataset_v7['id'] == -1].shape)\n",
    "print(dataset_v7[dataset_v7['id'] != -1].shape)\n",
    "\n",
    "dataset_v7.to_csv('json_cleaned.csv', header=True, sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
